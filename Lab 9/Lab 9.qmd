---
title: "Lab 9"
author: "Hanin Almodaweb"
format: html
theme: journal
embed-resources: true
---

```{r setup, message=FALSE, echo=FALSE, warning=FALSE}
library(microbenchmark)
library(parallel)
```


## Problem 1: Vectorization
1. This function generates an `n x k` dataset with all its entries drawn from a Poission distribution with mean `lambda`.
```{r}
# Original version
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  for (i in 1:n) {
    x <- rbind(x, rpois(k, lambda))
  }
  return(x)
}

# Optimized version
fun1alt <- function(n = 100, k = 4, lambda = 4) {
  matrix(rpois(n * k, lambda), nrow = n, ncol = k)
}

```

Show that `fun1alt` generates a matrix with the same dimensions as `fun1` and that the values inside the two matrices follow similar distributions. Then check the speed of the two functions with the following code:

```{r check}
# Generate outputs with a fixed seed for reproducibility
set.seed(123)
out1 <- fun1(100, 4, 4)

set.seed(123)
out2 <- fun1alt(100, 4, 4)

# Check if dimensions are the same
cat("Dimensions match:", all(dim(out1) == dim(out2)), "\n")

# Compare means and variances to check if values are drawn from similar distributions
cat("Mean difference:", abs(mean(out1) - mean(out2)), "\n")
cat("Variance difference:", abs(var(as.vector(out1)) - var(as.vector(out2))), "\n")


# Benchmarking
microbenchmark::microbenchmark(
  fun1(),
  fun1alt()
)
```
*The optimized function fun1alt() is significantly faster than the original fun1() in terms of improvement in execution time.* 

2.  This function finds the maximum value of each column of a matrix (hint: check out the `max.col()` function).
```{r}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2alt <- function(x) {
  x[cbind(max.col(t(x)), 1:ncol(x))]
  
}
```

*Both functions create the same output*

Show that both functions return the same output for a given input matrix, `x`. Then check the speed of the two functions.
```{r check 2}
# Ensure both functions return the same result
out1 <- fun2(x)
out2 <- fun2alt(x)

cat("Outputs identical:", identical(out1, out2), "\n")

# Benchmark both functions
benchmark_results <- microbenchmark(
  fun2(x),
  fun2alt(x)
)

print(benchmark_results)
```

*The optimized function fun2alt() is significantly faster than the original fun2() in terms of execution time.* 

## Problem 2: Parallelization

We will now turn our attention to the statistical concept of
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)). Among its many uses, non-parametric bootstrapping allows us to obtain confidence intervals for parameter estimates without relying on parametric assumptions. Don't worry if these concepts are unfamiliar, we only care about the computation methods in this lab, not the statistics.

The main assumption is that we can approximate the results of many repeated experiments by resampling observations from our original dataset, which reflects the population. 

1. This function implements a serial version of the bootstrap. Edit this function to parallelize the `lapply` loop, using whichever method you prefer. Rather than specifying the number of cores to use, use the number given by the `ncpus` argument, so that we can test it with different numbers of cores later.
```{r}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n * R, TRUE), nrow = n, ncol = R)

  # Set up the parallel cluster
  cl <- makeCluster(ncpus)
  on.exit(stopCluster(cl))  

  # Export the required objects/functions to the cluster
  clusterExport(cl, varlist = c("dat", "idx", "stat"), envir = environment())

  # Parallelize the lapply loop
  ans <- parLapply(cl, seq_len(R), function(i) {
    stat(dat[idx[, i], , drop = FALSE])
  })

  # Convert the list into a matrix
  ans <- do.call(rbind, ans)

  return(ans)
}
```

2. Once you have a version of the `my_boot()` function that runs on multiple cores, check that it provides accurate results by comparing it to a parametric model:
```{r}
# Bootstrap of an OLS
my_stat <- function(d) coef(lm(y ~ x, data=d))

# DATA SIM
set.seed(1)
n <- 500; R <- 1e4

x <- cbind(rnorm(n)); y <- x*5 + rnorm(n)

# Checking if we get something similar as lm
ans0 <- confint(lm(y~x))
ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 2L)

# You should get something like this
##                   2.5%      97.5%
## (Intercept) -0.1372435 0.05074397
## x            4.8680977 5.04539763

##                  2.5 %     97.5 %
## (Intercept) -0.1379033 0.04797344
## x            4.8650100 5.04883353

t(apply(ans1, 2, quantile, c(.025,.975)))
ans0
```

*The results indicate that the bootstrap confidence intervals obtained from the parallelized my_boot() function (ans1) are close to the parametric model's confidence intervals (ans0).* 

3.  Check whether your version actually goes faster when it's run on multiple cores (since this might take a little while to run, we'll use `system.time` and just run each version once, rather than `microbenchmark`, which would run each version 100 times, by default):
```{r benchmark-problem3}
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 1L))
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 2L))
```

*The benchmarking results demonstrate that the parallelized version of the my_boot() function significantly reduces computation time when using two cores compared to a single core.*